title: NPFL129, Lecture 5
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Derivation of Softmax, k-NN

## Milan Straka

### November 02, 2019

---
section: LagrangeM
# Lagrange Multipliers ‚Äì Equality Constraints

![w=28%,f=right](lagrange_equalities.svgz)

Given a funtion $J(‚Üíx)$, we can find a maximum with respect to a vector
$‚Üíx ‚àà ‚Ñù^d$, by investigating the critical points $‚àá_‚Üíx J(‚Üíx) = 0$.

~~~
Consider now finding maximum subject to a constraint $g(‚Üíx) = 0$.

~~~
- Note that $‚àá_‚Üíx g(‚Üíx)$ is orthogonal to the surface of the constraint, because
  if $‚Üíx$ and a nearby point $‚Üíx+‚ÜíŒµ$ lie on the surface, from the Taylor
  expansion $g(‚Üíx+‚ÜíŒµ) ‚âà g(‚Üíx) + ‚ÜíŒµ^T ‚àá_‚Üíx g(‚Üíx)$ we get $‚ÜíŒµ^T ‚àá_‚Üíx g(‚Üíx) ‚âà 0$.

~~~
- In the seeked maximum, $‚àá_‚Üíx f(‚Üíx)$ must also be orthogonal to the constraint
  surface (or else moving in the direction of the derivative would increase the
  value).

~~~
- Therefore, there must exist $Œª$ such that $‚àá_‚Üíx f + Œª‚àá_‚Üíx g = 0$.

---
# Lagrange Multipliers ‚Äì Equality Constraints

![w=28%,f=right](lagrange_equalities.svgz)

We therefore introduce the _Lagrangian function_
$$L(‚Üíx, Œª) ‚âù f(‚Üíx) + Œªg(‚Üíx).$$

~~~
We can then find the maximum under the constraint by inspecting critical points
of $L(‚Üíx, Œª)$ with respect to both $‚Üíx$ and $Œª$:
- $\frac{‚àÇL}{‚àÇŒª} = 0$ leads to $g(‚Üíx)=0$;
- $\frac{‚àÇL}{‚àÇ‚Üíx} = 0$ is the previously derived $‚àá_‚Üíx f + Œª‚àá_‚Üíx g = 0$.

~~~
If there are multiple equality constraints, we can use induction; therefore,
every constraint gets its own $Œª$.

---
section: NAsMaxEnt
class: dbend
# Calculus of Variations

Many optimization techniques depend on minimizing a function $J(‚Üíw)$ with
respect to a vector $‚Üíw ‚àà ‚Ñù^d$, by investigating the critical points $‚àá_‚Üíw J(‚Üíw) = 0$.

~~~
A function of a function, $J[f]$, is known as a **functional**, for example
entropy $H[‚ãÖ]$.

~~~
Similarly to partial derivatives, we can take **functional derivatives** of
a functional $J[f]$ with respect to individual values $f(‚Üíx)$ for all points
$‚Üíx$. The functional derivative of $J$ with respect to a function $f$ in a point
$‚Üíx$ is denoted as
$$\frac{‚àÇ}{‚àÇf(‚Üíx)} J.$$

~~~
For this course, we use only the following theorem stating that for
all differentiable functions $f$ and differentiable functions $g(y=f(‚Üíx), ‚Üíx)$ with
continuous derivatives, it holds that
$$\frac{‚àÇ}{‚àÇf(‚Üíx)} ‚à´g(f(‚Üíx), ‚Üíx) \d‚Üíx = \frac{‚àÇ}{‚àÇy} g(y, ‚Üíx).$$

---
class: dbend
# Calculus of Variations

An intuitive view is to think about $f(‚Üíx)$ as a vector of uncountably many
elements (for every value $‚Üíx)$. In this interpretation the result is analogous
to computing partial derivatives of a vector $‚Üíw ‚àà ‚Ñù^d$:
$$\frac{‚àÇ}{‚àÇw_i} ‚àë_j g(w_j, ‚Üíx) = \frac{‚àÇ}{‚àÇw_i} g(w_i, ‚Üíx).$$
$$\frac{‚àÇ}{‚àÇf(‚Üíx)} ‚à´g(f(‚Üíx), ‚Üíx) \d‚Üíx = \frac{‚àÇ}{‚àÇy} g(y, ‚Üíx).$$

---
class: dbend
# Function with Maximum Entropy

What distribution over $‚Ñù$ maximizes entropy $H[p] = -ùîº_x \log p(x)$?

~~~
For continuous values, the entropy is an integral $H[p] = -‚à´p(x) \log p(x) \d x$.

~~~
We cannot just maximize $H$ with respect to a function $p$, because:
- the result might not be a probability distribution ‚Äì we need to add
  a constraint that $‚à´p(x) \d x=1$;
~~~
- the problem is underspecified because a distribution can be shifted without
  changing entropy ‚Äì we add a constraint $ùîº[x] = Œº$;
~~~
- because entropy increases as variance increases, we ask which distribution
  with a _fixed_ variance $œÉ^2$ has maximum entropy ‚Äì adding a constraint
  $\Var(x) = œÉ^2$.

---
class: dbend
# Function with Maximum Entropy

Lagrangian of all the constraints and the entropy function is
$$L(p; Œº, œÉ^2) = Œª_1 \Big(‚à´p(x) \d x - 1\Big) + Œª_2 \big(ùîº[x] - Œº\big) + Œª_3\big(\Var(x) - œÉ^2\big) + H[p].$$

~~~
By expanding all definitions to integrals, we get
$$\begin{aligned}
L(p; Œº, œÉ^2) =& ‚à´\Big(Œª_1 p(x) + Œª_2 p(x) x + Œª_3 p(x) (x - Œº)^2 - p(x)\log p(x) \Big) \d x - \\
              & -Œª_1 - Œº Œª_2 - œÉ^2Œª_3.
\end{aligned}$$

~~~
The functional derivative of $L$ is:
$$\frac{‚àÇ}{‚àÇp(x)} L(p; Œº, œÉ^2) = Œª_1 + Œª_2 x + Œª_3 (x - Œº)^2 - 1 - \log p(x) = 0.$$

---
class: dbend
# Function with Maximum Entropy
Rearrangint the functional derivative of $L$:

$$\frac{‚àÇ}{‚àÇp(x)} L(p; Œº, œÉ^2) = Œª_1 + Œª_2 x + Œª_3 (x - Œº)^2 - 1 - \log p(x) = 0.$$

we obtain
$$p(x) = \exp\Big(Œª_1 + Œª_2 x + Œª_3 (x-Œº)^2 - 1\Big).$$

~~~
We can verify that setting $Œª_1 = 1 - \log œÉ \sqrt{2œÄ}$, $Œª_2=0$ and $Œª_3=-1/(2œÉ^2)$
fulfils all the constraints, arriving at
$$p(x) = ùìù(x; Œº, œÉ^2).$$

---
section: SoftMax
class: dbend
# Derivation of Softmax using Maximum Entropy

Let $ùïè = \{(‚Üíx_1, t_1), (‚Üíx_2, t_2), ‚Ä¶, (‚Üíx_N, t_N)\}$ be training data
of a $K$-class classification, with $‚Üíx_i ‚àà ‚Ñù^D$ and $t_i ‚àà \{1, 2, ‚Ä¶, K\}$.

~~~
We want to model it using a function $œÄ: ‚Ñù^D ‚Üí ‚Ñù^K$
so that $œÄ(‚Üíx)$ gives a distribution of classes for input $‚Üíx$.

~~~
We impose the following conditions on $œÄ$:
- $$œÄ(‚Üíx)_j ‚â• 0$$
~~~
- $$‚àë_{j=1}^K œÄ(‚Üíx)_j = 1$$
~~~
- $$‚àÄ_{k ‚àà \{1, 2, ‚Ä¶, D\}}, ‚àÄ_{j ‚àà \{1, 2, ‚Ä¶, K\}}: ‚àë_{i=1}^N œÄ(‚Üíx_i)_j x_{i,k} = ‚àë_{i=1}^N \Big[t_i == j\Big] x_{i,k}$$

---
class: dbend
# Derivation of Softmax using Maximum Entropy

There are many such $œÄ$, one particularly bad is
$$œÄ(‚Üíx) = \begin{cases}
  t_i&\textrm{if there exists }i: ‚Üíx_i = ‚Üíx, \\
  0&\textrm{otherwise}.\end{cases}$$

~~~
Therefore, we want to find a more general $œÄ$ ‚Äì we will aim for one with maximum
entropy.

---
class: dbend
# Derivation of Softmax using Maximum Entropy

We therefore want to maximize $-‚àë_{i=1}^N ‚àë_{j=1}^K œÄ(‚Üíx_i)_j \log(œÄ(‚Üíx_i)_j)$
given
- $œÄ(‚Üíx)_j ‚â• 0$,
- $‚àë_{i=j}^K œÄ(‚Üíx)_j = 1$,
- $‚àÄ_{k ‚àà \{1, ‚Ä¶, D\}}, ‚àÄ_{j ‚àà \{1, ‚Ä¶, K\}}: ‚àë_{i=1}^N œÄ(‚Üíx_i)_j x_{i,k} = ‚àë_{i=1}^N \big[t_i == j\big] x_{i,k}$.

~~~
We therefore form a Lagrangian (ignoring the first inequality constraint):
$$\begin{aligned}
L =& ‚àë_{k=1}^D ‚àë_{j=1}^K Œª_{k,j} \Big(‚àë_{i=1}^N œÄ(‚Üíx_i)_j x_{i,k} - \big[t_i == j\big] x_{i,k}\Big)\\
   & +‚àë_{i=1}^N Œ≤_i \Big(‚àë_{j=1}^K œÄ(‚Üíx_i)_j - 1\Big) \\
   & -‚àë_{i=1}^N ‚àë_{j=1}^K œÄ(‚Üíx_i)_j \log(œÄ(‚Üíx_i)_j)
\end{aligned}$$

---
class: dbend
# Derivation of Softmax using Maximum Entropy

We now compute partial derivatives of the Lagrangian, notably the values
$$\frac{‚àÇ}{‚àÇœÄ(‚Üíx_i)_j}L.$$

~~~
We arrive at
$$\frac{‚àÇ}{‚àÇœÄ(‚Üíx_i)_j}L = ‚ÜíŒª_{*,j} ‚Üíx_i + Œ≤_i - \log(œÄ(‚Üíx_i)_j) - 1.$$

~~~
Setting the Lagrangian to zero, we get $‚ÜíŒª_{*,j} ‚Üíx_i + Œ≤_i - \log(œÄ(‚Üíx_i)_j) - 1 = 0,$
which we rewrite to
$$œÄ(‚Üíx_i)_j = e^{‚ÜíŒª_{*,j}‚Üíx_i + Œ≤_i - 1}.$$

~~~
Such a forms guarantees $œÄ(‚Üíx_i)_j > 0$, which we did not include in the
conditions.

---
class: dbend
# Derivation of Softmax using Maximum Entropy

In order to find out the $Œ≤_i$ values, we turn to the constraint
$$‚àë_j œÄ(‚Üíx_i)_j = ‚àë_j e^{‚ÜíŒª_{*,j}‚Üíx_i +Œ≤_i-1} = 1,$$
from which we get
$$e^{Œ≤_i} = \frac{1}{‚àë_j e^{‚ÜíŒª_{*,j}‚Üíx_i-1}},$$

~~~
yielding
$$œÄ(‚Üíx_i)_j = \frac{e^{‚ÜíŒª_{*,j}‚Üíx_i}}{‚àë_k e^{‚ÜíŒª_{*,k}‚Üíx_i}}.$$

---
section: F-score
# F1-score

When evaluating binary classification, we have used **accuracy** so far.

~~~
![w=36%,f=right](true_false_positives.svgz)

However, there are other metric we might want to consider. One of them is
$F_1$-score.

~~~
| |Target<br>positive|Target<br>negative|
|-|------------------|------------------|
|Predicted<br>positive| **True Positive** (**TP**) | **False Positive** (**FP**) |
|Predicted<br>negative| **False Negative** (**FN**) | **True Negative** (**TN**) |

~~~
Accuracy can be computed as
$$\mathrm{accuracy} = \frac{\mathit{TP}+\mathit{TN}}{\mathit{TP}+\mathit{TN}+\mathit{FP}+\mathit{FN}}.$$

---
# F1-score

![w=50%,h=center,mw=36%,f=right](true_false_positives.svgz)

| |Target<br>positive|Target<br>negative|
|-|------------------|------------------|
|Predicted<br>positive| **True Positive** (**TP**) | **False Positive** (**FP**) |
|Predicted<br>negative| **False Negative** (**FN**) | **True Negative** (**TN**) |

~~~
![w=100%,h=center,mw=36%,f=right](precision_recall.svgz)

We define **precision** (percentage of correct predictions in the predicted examples)
and **recall** (percentage of correct predictions in the gold examples) as
$$\begin{aligned}
  \mathrm{precision} =& \frac{\mathit{TP}}{\mathit{TP}+\mathit{FP}}, \\
  \mathrm{recall} =& \frac{\mathit{TP}}{\mathit{TP}+\mathit{FN}}.
\end{aligned}$$

---
# F1-score

![w=47%,f=right](roc_pr.png)

The precision and recall go ‚Äúagainst each other‚Äù in a sense.

~~~
We therefore define **$\boldsymbol{F_1}$-score** as a harmonic mean of precision and recall:
$$\begin{aligned}
  F_1 =& \frac{2}{\mathrm{precision}^{-1} + \mathrm{recall}^{-1}} \\
      =& \frac{2 ‚ãÖ \mathrm{precision} ‚ãÖ \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}.
\end{aligned}$$

~~~
The $F_1$ score can be generalized to $F_Œ≤$ score, where recall is $Œ≤$ times
more important than precision; $F_2$ favoring recall and $F_{0.5}$ favoring
precision are commonly used.

$$ F_Œ≤ = \frac{(1 + Œ≤)^2 ‚ãÖ \mathrm{precision} ‚ãÖ \mathrm{recall}}{Œ≤^2 ‚ãÖ \mathrm{precision} + \mathrm{recall}}.$$

---
# Precision-Recall Curve, ROC Curve

<div style="width: 37%; float: right">
![w=100%](roc_pr.png)
![w=100%](roc_curve.svgz)
</div>

Changing the threshold in logistic regression allows us to trade off precision
for recall and vice versa. Therefore, we can tune it on the development set to
achieve highest possible $F_1$ score, if required.

~~~
Note that $F_1$ score does not consider true negatives at all.

Therefore, apart from the precision-recall curve, the **Receiver Operating
Characteristic** (ROC) curve is also used to describe binary classifiers. In the
ROC curve, we consider:
- _true positive rate_ (recall; probability of detection);
- _false positive rate_ (probability of false alarm).

~~~
When evaluating a binary classifier, the **area under curve** (AUC) is sometimes
also used as a metric.

---
section: (Non)ParametricMethods
# Parametric and Nonparametric Methods

All the machine learning models which we discussed so far are **parametric**,
because they use a _fixed_ number of parameters (usually depending on the
number of features, $K$ for multiclass classification, hidden layer in MLPS, ‚Ä¶).

~~~
However, there also exist **nonparametric** methods. Even if the name seems to
suggest they do not have any parameters, they have a non-fixed number of
parameters, because the number of parameters usually depend on the size of the
training data.

---
section: k-NN
# k-Nearest Neighbors

A simple but sometimes effective nonparametric method for both classification
and regression is **k-nearest neighbors** algorithm.

**Work in progress**

~~~
- $k=1$

~~~
- $k > 1$

~~~
- weighted variant
