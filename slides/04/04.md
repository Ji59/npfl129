title: NPFL129, Lecture 4
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Multiclass Logistic Regression, Multilayer Perceptron

## Milan Straka

### October 26, 2020

---
section: MulticlassLogisticRegression
# Multiclass Logistic Regression

To extend the binary logistic regression to a multiclass case with $K$ classes, we:
- Generate multiple outputs, notably $K$ outputs, each with its own set of
  weights, so that
  $$→f(→x; ⇉W) = ⇉W →x,\textrm{~~~or in other words~~~}→f(→x; ⇉W)_i = →W_i^T →x.$$

~~~
- Generalize the sigmoid function to a $\softmax$ function, such that
  $$\softmax(→z)_i = \frac{e^{z_i}}{∑_j e^{z_j}}.$$

~~~
  Note that the original sigmoid function can be written as
  $$σ(x) = \softmax\big([x~~0]\big)_0 = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}}.$$

~~~
The resulting classifier is also known as **multinomial logistic regression**,
**maximum entropy classifier** or **softmax regression**.

---
# Multiclass Logistic Regression

Note that as defined, the multiclass logistic regression is overparametrized.
It is possible to generate only $K-1$ outputs and define $z_K = 0$, which is
the approach used in binary logistic regression.

~~~
In this settings, analogously to binary logistic regression, we can recover the
interpretation of the model outputs $→f(→x; ⇉W)$ (i.e., the $\softmax$ inputs)
as _logits_:

$$f(→x; ⇉W)_i = \log\left(\frac{P(C_i | →x; →w)}{P(C_K | →x; →w)}\right).$$

---
# Multiclass Logistic Regression

Using the $\softmax$ function, we naturally define that
$$P(C_i | →x; ⇉W) = \softmax(⇉W_i^T →x)i = \frac{e^{⇉W_i^T →x}}{∑_j e^{⇉W_j^T →x}}.$$

~~~
We can then use MLE and train the model using stochastic gradient descent.

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, 1, …, K-1\}$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g ← - \tfrac{1}{N} ∑_i ∇_→w \log(P(C_{t_i} | →x_i; →w)$
  - $→w ← →w - α→g$
</div>

---
# Multiclass Logistic Regression

![w=41%,f=right](classification_convex_regions.svgz)

Note that the decision regions of the binary/multiclass logistic regression are
convex (and therefore connected).

~~~
To see this, consider $→x_A$ and $→x_B$ in the same decision region $R_k$.

~~~
Any point $→x$ lying on the line connecting them is their linear combination,
$→x = λ→x_A + (1-λ)→x_B$,
~~~
and from the linearity of $→f(→x) = ⇉W→x$ it follows that
$$→f(→x) = λ→f(→x_A) + (1-λ)→f(→x_B).$$

~~~
Given that $f_k(→x_A)$ was the largest among $→f(→x_A)$ and also
given that $f_k(→x_B)$ was the largest among $→f(→x_B)$, it must
be the case that $f_k(→x)$ is the largest among all $→f(→x)$.
