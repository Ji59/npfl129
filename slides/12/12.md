title: NPFL129, Lecture 12
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gaussian Mixture,<br>Bias-Variance Tradeoff

## Milan Straka

### December 21, 2020

---
section: GaussianMixture
# Gaussian Mixture

Let $→x_1, →x_2, …, →x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $→x_i ∈ ℝ^D$. Let $K$, the number of target clusters,
be given.

~~~
Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussian in the form
$$p(→x) = ∑_{k=1}^K π_k 𝓝(→x | →μ_k, ⇉Σ_k).$$
Therefore, each cluster is parametrized as $𝓝(→x | →μ_k, ⇉Σ_k)$.

~~~
Let $→z ∈ \{0, 1\}^K$ be a $K$-dimensional random variable, such that exactly
one $z_k$ is 1, denoting to which cluster a training example belongs.
~~~
Let the marginal distribution of $z_k$ be
$$p(z_k = 1) = π_k.$$

~~~
Therefore, $p(→z) = ∏_k π_k^{z_k}$.

---
# Gaussian Mixture

![w=6%,f=right](mog_latent.svgz)

We can write
$$p(→x) = ∑_{→z} p(→z) p(→x | →z) = ∑_{k=1}^K π_k 𝓝(→x | →μ_k, ⇉Σ_k)$$
~~~
and the probability of the whole clustering is therefore
$$\log p(⇉X | →π, →μ, →Σ) = ∑_{i=1}^N \log \left(∑_{k=1}^K π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)\right).$$

~~~
To fit a Gaussian mixture model, we start with maximum likelihood estimation and
minimize
$$𝓛(⇉X) = - ∑_i \log ∑_{k=1}^K π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)$$

---
# Gaussian Mixture

The derivative of the loss with respect to $→μ_k$ gives
$$\frac{∂𝓛(⇉X)}{∂→μ_k} = - ∑_i \frac{π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)} ⇉Σ_k^{-1} \big(→x_i - →μ_k\big)$$

~~~
Denoting $r(z_{i,k}) = \frac{π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)}$,
setting the derivative equal to zero and multiplying by $⇉Σ_k^{-1}$, we get
$$→μ_k = \frac{∑_i r(z_{i,k}) →x_i}{∑_i r(z_{i,k})}.$$

~~~
The $r(z_{i,k})$ are usually called **responsibilities** and denote the
probability $p(z_k = 1 | →x_i)$. Note that the responsibilities depend on
$→μ_k$, so the above equation is not an analytical solution for $→μ_k$, but
can be used as an _iterative_ algorithm for converting to local optimum.

---
# Gaussian Mixture

For $⇉Σ_k$, we again compute the derivative of the loss, which is technically
complicated (we need to compute derivative with respect a matrix, and also we
need to differentiate matrix determinant) and results in an analogous equation
$$→Σ_k = \frac{∑_i r(z_{i,k}) (→x_i - →μ_k) (→x_i - →μ_k)^T}{∑_i r(z_{i,k})}.$$

~~~
To minimize the loss with respect to $→π$, we need to include the constraint
$∑_k π_k = 1$, so we form a Lagrangian $𝓛(⇉X) + λ\left(∑\nolimits_k π_k - 1\right)$,
and get
$$0 = ∑_i \frac{𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)} + λ,$$
~~~
from which we get $π_k ∝ ∑_i r(z_{i,k})$ and therefore
$$π_k = 1/N ⋅ ∑\nolimits_i r(z_{i,k}).$$

---
# Gaussian Mixture

<div class="algorithm">

**Input**: Input points $→x_1, …, →x_N$, number of clusters $K$.

- Initialize $→μ_k, ⇉Σ_k$ and $π_k$. It is common to start by running the
  K-Means algorithm to obtain $z_{i,k}$, set $r(z_{i,k}) ← z_{i,k}$
  and use the **M step** below.

- Repeat until convergence (or until patience runs out):
  - **E step**. Evaluate the responsibilities as
    $$r(z_{i,k}) = \frac{π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)}.$$
  - **M step**. Maximize the log-likelihood by setting

    $$→μ_k = \frac{∑_i r(z_{i,k}) →x_i}{∑_i r(z_{i,k})},~~
      →Σ_k = \frac{∑_i r(z_{i,k}) (→x_i - →μ_k) (→x_i - →μ_k)^T}{∑_i r(z_{i,k})},~~
      π_k = \frac{∑_i r(z_{i,k})}{N}.$$

</div>

---
# Gaussian Mixture

![w=75%,h=center](../11/mog_data.svgz)
![w=75%,h=center](../11/mog_illustration.svgz)

---
# Gaussian Mixture

![w=75%,h=center](mog_example.svgz)

---
section: EM
# EM Algorithm

The algorithm for estimating the Gaussian mixture is an example of an **EM
algorithm**.

~~~
The **EM algorithm** algorithm can be used when given a a joint distribution
$p(⇉X, ⇉Z | →w)$ over observed variables $⇉X$ and latent (hidden, unseen)
variabled $⇉Z$, parametried by $→w$,
~~~
we maximize
$$\log p(⇉X; →w) = \log \left(∑_⇉Z p(⇉X, ⇉Z; →w)\right)$$
with respect to $→w$.

~~~
The main idea is to approximate the sum over all latent variables by
the expectation of the joint probability under the posterior latent
variable distribution $p(⇉Z | ⇉X; →w)$.

~~~
Usually, the latent variables $⇉Z$ indicate membership of the data in one of the
set of groups.

---
# EM Algorithm

<div class="algorithm">

- Initialize the parameters $→w^\mathrm{old}$.

- Repeat until convergence (or until patience runs out):
  - **E step**. Evaluate
    $$Q(→w | →w^\mathrm{old}) = 𝔼_{⇉Z|⇉X, →w} \big[\log p(⇉X, ⇉Z; →w)\big].$$
  - **M step**. Maximize the log-likelihood by computing
    $$→w^\mathrm{new} ← \argmax_→w Q(→w | →w^\mathrm{old}).$$
</div>
