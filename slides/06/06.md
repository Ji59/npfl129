title: NPFL129, Lecture 6
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Kernel Methods, SVM

## Milan Straka

### November 09, 2019

---
section: KernelLR
# Kernel Linear Regression

Consider linear regression with cubic features
$$φ(→x) = \scriptsize\begin{bmatrix} 1 \\ x_1 \\ x_2 \\ … \\ x_1^2 \\ x_1x_2 \\ … \\ x_2x_1 \\ … \\ x_1^3 \\ x_1^2x_2 \\ … \end{bmatrix}.$$

~~~
The SGD update for linear regression is then
$$→w ← →w + α\big(t - →w^T φ(→x)\big) φ(→x).$$

---
# Kernel Linear Regression

When dimensionality of input is $D$, one step of SGD takes $𝓞(D^3)$.

~~~
Surprisingly, we can do better under some circumstances. We start by
noting that we can write the parameters $→w$ as a linear combination
of the input features $φ(→x_i)$.

~~~
By induction, $→w = 0 = ∑_i 0 ⋅ φ(→x_i)$, and assuming $→w = ∑_i β_i ⋅ φ(→x_i)$,
after a SGD update we get
$$\begin{aligned}
→w ←& →w + α∑_i \big(t_i - →w^T φ(→x_i)\big) φ(→x_i)\\
   =& ∑_i \Big(β_i + α \big(t_i - →w^T φ(→x_i)\big)\Big) φ(→x_i).
\end{aligned}$$

~~~
A individual update is $β_i ← β_i + α\Big(t_i - →w^T φ(→x_i)\Big)$, and
substituting for $→w$ we get
$$β_i ← β_i + α\Big(t_i - ∑\nolimits_j β_j φ(→x_j)^T φ(→x_i)\Big).$$

---
# Kernel Linear Regression

We can formulate an alternative linear regression algorithm (a so-called
**dual formulation**):

<div class="algorithm">

**Input**: Dataset ($⇉X = \{→x_1, →x_2, …, →x_N\} ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), learning rate $α ∈ ℝ^+$.<br>

- Set $β_i ← 0$
- Compute all values $K(→x_i, →x_j) = φ(→x_i)^T φ(→x_j)$
- Repeat until convergence
  - Update the coordinates, either according to a full gradient update:
    - $→β ← →β + α(→t-K→β)$
  - or alternatively use single-batch SGD, arriving at:
    - for $i$ in random permutation of $\{1, …, N\}$:
      - $β_i ← β_i + α\Big(t_i - ∑\nolimits_j β_j K(→x_i, →x_j)\Big)$
</div>

~~~
The predictions are then performed by computing
$$y(→x) = →w^T φ(→x) = ∑\nolimits_i β_i →φ(→x_i)^T →φ(→x).$$

---
section: Kernels
# Kernel Trick

A single SGD update of all $β_i$ then takes $𝓞(N^2)$, given that we can
evaluate scalar dot product of $φ(→x_j)^T φ(→x_i)$ quickly.

~~~
$$\begin{aligned}
φ(→x)^T φ(→z) =& 1 + ∑_i x_i z_i + ∑_{i,j} x_i x_j z_i z_j + ∑_{i,j,k} x_i x_j x_k z_i z_j z_k \\
              =& 1 + ∑_i x_i z_i + \Big(∑_i x_i z_i\Big)^2 + \Big(∑_i x_i z_i\Big)^3 \\
              =& 1 + →x^T →z + \big(→x^T →z\big)^2 + \big(→x^T →z\big)^3.
\end{aligned}$$

---
# Kernels

We define a _kernel_ corresponding to a feature map $φ$ as a function
$$K(→x, →z) ≝ φ(→x)^t φ(→z).$$

~~~
There is quite a lot of theory behind kernel construction. The most often used
kernels are:

~~~
- polynomial kernel or degree $d$
  $$K(→x, →z) = (γ →x^T→z)^d,$$
  which corresponds to a feature map generating all combinations of exactly
  $d$ input features;
~~~
- polynomial kernel or degree at most $d$
  $$K(→x, →z) = (γ →x^T→z + 1)^d,$$
  which corresponds to a feature map generating all combinations of up to
  $d$ input features;

---
# Kernels

- Gaussian (or RBF) kernel
  $$K(→x, →z) = e^{-γ||→x-→z||^2},$$
  corresponding to a scalar product in an infinite-dimensional space (it is
  in a sense a combination of polynomial kernels of all degrees, details will
  appear later).

---
section: SVM
# Support Vector Machines

Let us return to a binary classification task. The perceptron algorithm
guaranteed finding some separating hyperplane if it existed; we now consider
finding the one with _maximum margin_.

![w=100%,h=center](svm_margin.svgz)

---
# Support Vector Machines

Assume we have a dataset $⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1, 1\}^N$, feature map $φ$ and model
$$y(→x) ≝ →φ(→x)^T →w + b.$$

~~~
![w=30%,f=right](../03/binary_classification.svgz)

We already know that the distance of a point $→x_i$ to the decision boundary is
$$\frac{|y(→x_i)|}{||→w||} = \frac{t_i y(→x_i)}{||→w||}.$$

~~~
We therefore want to maximize
$$\argmax_{w,b} \frac{1}{||→w||} \min_i \big[t_i (→φ(→x)^T →w + b)\big].$$

However, this problem is difficult to optimize directly.

---
# Support Vector Machines

Because the model is invariant to multiplying $→w$ and $b$ by a constant, we can
say that for the points closest to the decision boundary, it will hold that
$$t_i y(→x_i) = 1.$$

~~~
Then for all the points we will have $t_i y(→x_i) ≥ 1$ and we can simplify
$$\argmax_{w,b} \frac{1}{||→w||} \min_i \big[t_i (→φ(→x)^T →w + b)\big]$$
to
$$\argmin_{w,b} \frac{1}{2} ||→w||^2 \textrm{~given that~~}t_i y(→x_i) ≥ 1.$$

---
section: KKT
# Lagrange Multipliers – Inequality Constraints

Given a funtion $J(→x)$, we can find a maximum with respect to a vector
$→x ∈ ℝ^d$, by investigating the critical points $∇_→x J(→x) = 0$.

We even know how to incorporate constraints of form $g(→x) = 0$.

~~~
![w=25%,f=right](lagrange_inequalities.svgz)

We now describe how to include inequality constraints $g(→x) ≥ 0$.

~~~
The optimum can either be attained for $g(→x) > 0$, when the constraint is said
to be _inactive_, or for $g(→x) = 0$, when the constraint is sayd to be
_active_.

~~~
In the inactive case, the maximum is again a critical point of the Langrangian,
with $λ=0$.
~~~
When maximum is on boundary, it corresponds to a critical point
with $λ≠0$ – but note that this time the sign of the multiplier matters, because
maximum is attained only when gradient of $f(→x)$ is oriented away from the region
$g(→x) ≥ 0$. We therefore require $∇f(→x) = - λ∇g(→x)$ for $λ>0$.

~~~
In both cases, $λ g(→x) = 0$.

---
section: KKT
# Karush-Khun-Tucker Conditions

![w=25%,f=right](lagrange_inequalities.svgz)

Therefore, the solution to a maximization problem of $f(x)$ subject to $g(→x)≥0$
can be found by inspecting all points where the derivation of the Lagrangian is zero,
subject to the following conditions:
$$\begin{aligned}
g(→x) &≥ 0 \\
λ &≥ 0 \\
λ g(→x) &= 0
\end{aligned}$$

~~~
# Necessary and Sufficient KKT Conditions

The above conditions are necessary conditions for a minimum. However, it can be
proven that in the following settings, the conditions are also **sufficient**:
- if the objective to optimize is a _convex_ function,
~~~
- the inequality constraings are continuously differentiable convex functions,
~~~
- the equality constraints are affine functions (linear functions with an
  offset).


---
section: Dual SVM Formulation
# Support Vector Machines

In order to solve the constrained problem of
$$\argmin_{w,b} \frac{1}{2} ||→w||^2 \textrm{~given that~~}t_i y(→x_i) ≥ 1,$$
~~~
we write the Lagrangian with multipliers $→a=(a_1, …, a_N)$ as
$$L = \frac{1}{2} ||→w||^2 - ∑_i a_i \big[t_i y(→x_i) - 1\big].$$

~~~
Setting the derivatives with respect to $→w$ and $b$ to zero, we get
$$\begin{aligned}
→w =& ∑_i a_i t_iφ(→x_i) \\
 0 =& ∑_i a_i t_i \\
\end{aligned}$$

---
# Support Vector Machines

Substituting these to the Lagrangian, we get
$$L = ∑_i a_i -  \frac{1}{2} ∑_i ∑_j a_i a_j t_i t_j K(→x_i, →x_j)$$
with respect to the constraints $∀_i: a_i ≥ 0$, $∑_i a_i t_i = 0$
and kernel $K(→x, →z) = φ(→x)^T φ(→z).$

~~~
The solution of this Lagrangian will fulfil the KKT conditions, meaning that
$$\begin{aligned}
a_i &≥ 0 \\
t_i y(→x_i) - 1 &≥ 0 \\
a_i \big(t_i y(→x_i) - 1\big) &= 0.
\end{aligned}$$

~~~
Therefore, either a point is on a boundary, or $a_i=0$. Given that the
predictions for point $→x$ are given by $y(→x) = ∑ a_i t_i K(→x, →x_i) + b$,
we need to keep only the points on the boundary, the so-called **support vectors**.

---
# Support Vector Machines

The dual formulation allows us to use non-linear kernels.

![w=100%](svm_gaussian.svgz)
